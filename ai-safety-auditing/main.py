"""
ä¸»ç¨‹å¼ï¼šä¸ä½¿ç”¨API,å®Œæ•´çš„ AI å®‰å…¨å¯©è¨ˆæµç¨‹
"""

import json
import os
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

from src.generator.attack_generator import AttackGenerator
from src.target.config_manager import ConfigManager
from src.judge.safety_judge import SafetyJudge
from src.evaluation.metrics import MetricsCalculator

# æª¢æŸ¥æ˜¯å¦æœ‰ tqdm
try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    print("âš ï¸  æœªå®‰è£ tqdmï¼Œé€²åº¦æ¢å°‡ä¸é¡¯ç¤º")
    print("   åŸ·è¡Œï¼špip install tqdm")


def main():
    """ä¸»ç¨‹å¼ï¼šå®Œæ•´çš„ Red Team æ¸¬è©¦æµç¨‹"""
    
    print("\n" + "=" * 70)
    print("ğŸš€ AI Safety Auditing System")
    print("   Responsible AI Red Team Testing")
    print("=" * 70 + "\n")
    
    # ==================== ç¬¬ 1 æ­¥ï¼šç”Ÿæˆæ”»æ“Šæç¤ºè© ====================
    print("ğŸ“ æ­¥é©Ÿ 1/5: ç”Ÿæˆæ”»æ“Šæç¤ºè©")
    print("-" * 70)
    
    generator = AttackGenerator()
    
    # ç”Ÿæˆæ”»æ“Šï¼ˆå¯èª¿æ•´åƒæ•¸ï¼‰
    attacks = generator.generate_attacks(
        category="all",      # "all" æˆ–æŒ‡å®šé¡åˆ¥
        count=20,            # æ¯å€‹é¡åˆ¥çš„æ•¸é‡
        use_llm=True         # ä½¿ç”¨ LLM ç”Ÿæˆï¼ˆFalse å‰‡ç”¨æ¨¡æ¿ï¼‰
    )
    
    print(f"âœ… å·²ç”Ÿæˆ {len(attacks)} å€‹æ”»æ“Šæç¤ºè©")
    print(f"   é¡åˆ¥åˆ†å¸ƒ:")
    
    # é¡¯ç¤ºé¡åˆ¥åˆ†å¸ƒ
    from collections import Counter
    categories = Counter([a['category'] for a in attacks])
    for cat, count in categories.items():
        print(f"     - {cat}: {count}")
    
    # å„²å­˜æ”»æ“Š
    attacks_file = Path("data/attacks/generated_attacks.json")
    attacks_file.parent.mkdir(parents=True, exist_ok=True)
    generator.save_attacks(attacks, str(attacks_file))
    
    # ==================== ç¬¬ 2 æ­¥ï¼šè¼‰å…¥ç›®æ¨™æ¨¡å‹ ====================
    print(f"\nğŸ¯ æ­¥é©Ÿ 2/5: è¼‰å…¥ç›®æ¨™æ¨¡å‹")
    print("-" * 70)
    
    # æª¢æŸ¥é…ç½®æª”æ˜¯å¦å­˜åœ¨
    config_file = Path("config/models_config.json")
    if not config_file.exists():
        print("âš ï¸  é…ç½®æª”ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¯„ä¾‹é…ç½®...")
        example_file = Path("config/models_config.example.json")
        if example_file.exists():
            import shutil
            shutil.copy(example_file, config_file)
            print(f"âœ… å·²è¤‡è£½ç¯„ä¾‹é…ç½®è‡³ {config_file}")
            print(f"âš ï¸  è«‹ç·¨è¼¯é…ç½®æª”ä¸¦è¨­å®š API Keys")
        else:
            print("âŒ æ‰¾ä¸åˆ°ç¯„ä¾‹é…ç½®æª”")
            return
    
    config_manager = ConfigManager(str(config_file))
    target_models = config_manager.load_and_create_models()
    
    if not target_models:
        print("âŒ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè«‹æª¢æŸ¥é…ç½®æª”")
        return
    
    print(f"âœ… å·²è¼‰å…¥ {len(target_models)} å€‹ç›®æ¨™æ¨¡å‹:")
    for model in target_models:
        print(f"   - {model.model_name}")
    
    # ==================== ç¬¬ 3 æ­¥ï¼šåˆå§‹åŒ– Judge ====================
    print(f"\nâš–ï¸  æ­¥é©Ÿ 3/5: åˆå§‹åŒ– Judge ç³»çµ±")
    print("-" * 70)
    
    judge = SafetyJudge()
    print(f"âœ… Judge å·²å°±ç·’")
    print(f"   æ¨¡å‹: {judge.model}")
    print(f"   æº«åº¦: {judge.temperature}")
    
    # ==================== ç¬¬ 4 æ­¥ï¼šåŸ·è¡Œæ‰¹é‡æ¸¬è©¦ ====================
    print(f"\nğŸ§ª æ­¥é©Ÿ 4/5: åŸ·è¡Œæ‰¹é‡æ¸¬è©¦")
    print("-" * 70)
    
    total_tests = len(attacks) * len(target_models)
    print(f"ç¸½æ¸¬è©¦æ•¸: {total_tests}")
    print(f"é ä¼°æ™‚é–“: {total_tests * 3 / 60:.1f} åˆ†é˜ï¼ˆç´„æ¯å€‹æ¸¬è©¦ 3 ç§’ï¼‰\n")
    
    all_results = []
    
    # å‰µå»ºé€²åº¦æ¢
    if HAS_TQDM:
        pbar = tqdm(total=total_tests, desc="æ¸¬è©¦é€²åº¦")
    
    for model_idx, model in enumerate(target_models):
        print(f"\næ¸¬è©¦æ¨¡å‹ {model_idx+1}/{len(target_models)}: {model.model_name}")
        model_results = []
        
        for attack_idx, attack in enumerate(attacks):
            # å‘¼å«ç›®æ¨™æ¨¡å‹
            response = model.generate_with_retry(attack["prompt"])
            
            # Judge è©•åˆ†
            if response.get("success"):
                evaluation = judge.evaluate(
                    attack["prompt"],
                    response.get("response", "")
                )
            else:
                # å¦‚æœæ¨¡å‹å‘¼å«å¤±æ•—ï¼Œè¨˜éŒ„éŒ¯èª¤
                evaluation = {
                    "score": None,
                    "is_safe": None,
                    "reasoning": f"æ¨¡å‹å‘¼å«å¤±æ•—: {response.get('error')}",
                    "error": response.get("error")
                }
            
            # è¨˜éŒ„çµæœ
            result = {
                "timestamp": datetime.now().isoformat(),
                "model": model.model_name,
                "attack_id": attack["id"],
                "attack_category": attack["category"],
                "attack_prompt": attack["prompt"],
                "model_response": response.get("response"),
                "evaluation": evaluation,
                "latency_ms": response.get("latency_ms", 0),
                "success": response.get("success", False)
            }
            
            model_results.append(result)
            all_results.append(result)
            
            # æ›´æ–°é€²åº¦æ¢
            if HAS_TQDM:
                pbar.set_postfix({
                    "æ¨¡å‹": model.model_name[:20],
                    "åˆ†æ•¸": evaluation.get("score", "N/A")
                })
                pbar.update(1)
            else:
                # ç°¡å–®çš„é€²åº¦é¡¯ç¤º
                if (attack_idx + 1) % 5 == 0:
                    print(f"  é€²åº¦: {attack_idx+1}/{len(attacks)}")
    
    if HAS_TQDM:
        pbar.close()
    
    print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼å…± {len(all_results)} å€‹æ¸¬è©¦")
    
    # ==================== ç¬¬ 5 æ­¥ï¼šè¨ˆç®—æŒ‡æ¨™èˆ‡å„²å­˜ ====================
    print(f"\nğŸ“Š æ­¥é©Ÿ 5/5: è¨ˆç®—è©•ä¼°æŒ‡æ¨™")
    print("-" * 70)
    
    # æ•´é«”æŒ‡æ¨™
    overall_evals = [r["evaluation"] for r in all_results]
    overall_metrics = MetricsCalculator.calculate_all_metrics(overall_evals)
    
    # æŒ‰æ¨¡å‹åˆ†çµ„è¨ˆç®—
    results_by_model = {}
    for model in target_models:
        model_results = [
            r for r in all_results 
            if r["model"] == model.model_name
        ]
        results_by_model[model.model_name] = [r["evaluation"] for r in model_results]
    
    model_comparison = MetricsCalculator.compare_models(results_by_model)
    
    # æŒ‰æ”»æ“Šé¡åˆ¥è¨ˆç®—
    category_metrics = MetricsCalculator.calculate_category_metrics(
        [r["evaluation"] for r in all_results if "attack_category" in r]
    )
    
    # ==================== å„²å­˜çµæœ ====================
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path("data/results")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. å„²å­˜è©³ç´°çµæœ
    detailed_file = results_dir / f"detailed_results_{timestamp}.json"
    with open(detailed_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    # 2. å„²å­˜æŒ‡æ¨™æ‘˜è¦
    # å„²å­˜æ¸¬è©¦é…ç½®å’Œæ•´é«”æŒ‡æ¨™
    metrics_summary = {
        "timestamp": timestamp,
        "config": {
            "total_attacks": len(attacks),
            "total_models": len(target_models),
            "total_tests": total_tests,
            "generator_model": generator.model,
            "judge_model": judge.model
        },
        "overall_metrics": overall_metrics,
        "model_comparison": model_comparison,
        "category_metrics": category_metrics
    }
    
    summary_file = results_dir / f"metrics_summary_{timestamp}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(metrics_summary, f, indent=2, ensure_ascii=False)
    
    # ==================== é¡¯ç¤ºçµæœæ‘˜è¦ ====================
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦")
    print("=" * 70)
    
    print(f"\næ•´é«”è¡¨ç¾ï¼š")
    print(f"  æ”»æ“ŠæˆåŠŸç‡ (ASR): {overall_metrics['asr']:.2%}")
    print(f"  å¹³å‡å®‰å…¨åˆ†æ•¸: {overall_metrics['average_score']:.2f}/5.0")
    print(f"  æ‹’çµ•ç‡: {overall_metrics['refusal_rate']:.2%}")
    
    print(f"\nå„æ¨¡å‹è¡¨ç¾ï¼š")
    for model_name, metrics in model_comparison["by_model"].items():
        print(f"\n  {model_name}:")
        print(f"    ASR: {metrics['asr']:.2%}")
        print(f"    å¹³å‡åˆ†æ•¸: {metrics['average_score']:.2f}/5.0")
        print(f"    æ‹’çµ•ç‡: {metrics['refusal_rate']:.2%}")
    
    print(f"\næ¨¡å‹æ’åï¼ˆä¾ ASRï¼Œè¶Šä½è¶Šå¥½ï¼‰ï¼š")
    for i, (model, asr) in enumerate(model_comparison["ranking"]["by_asr"], 1):
        print(f"  {i}. {model}: {asr}")
    
    print(f"\nå„æ”»æ“Šé¡åˆ¥è¡¨ç¾ï¼š")
    for category, metrics in category_metrics.items():
        print(f"  {category}: ASR={metrics['asr']:.2%}, å¹³å‡åˆ†æ•¸={metrics['avg_score']:.2f}")
    
    print("\n" + "=" * 70)
    print("ğŸ’¾ çµæœå·²å„²å­˜:")
    print(f"   è©³ç´°çµæœ: {detailed_file}")
    print(f"   æŒ‡æ¨™æ‘˜è¦: {summary_file}")
    print("=" * 70)
    
    print("\nâœ¨ æ¸¬è©¦å®Œæˆï¼")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ¸¬è©¦è¢«ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        print(f"\n\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
